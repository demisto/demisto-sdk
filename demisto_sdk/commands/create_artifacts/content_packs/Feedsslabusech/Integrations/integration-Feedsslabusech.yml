category: Data Enrichment & Threat Intelligence
commonfields:
  id: abuse.ch SSL Blacklist Feed
  version: -1
configuration:
- display: Services
  name: url
  options:
  - https://sslbl.abuse.ch/blacklist/sslipblacklist.csv
  - https://sslbl.abuse.ch/blacklist/sslipblacklist_aggressive.csv
  required: true
  type: 16
- display: Fetch indicators
  name: feed
  required: false
  type: 8
  defaultvalue: true
- additionalinfo: Indicators from this integration instance will be marked with this reputation
  defaultvalue: Bad
  display: Indicator Reputation
  name: feedReputation
  options:
  - None
  - Good
  - Suspicious
  - Bad
  required: false
  type: 18
- defaultvalue: B - Usually reliable
  display: Source Reliability
  name: feedReliability
  options:
  - A - Completely reliable
  - B - Usually reliable
  - C - Fairly reliable
  - D - Not usually reliable
  - E - Unreliable
  - F - Reliability cannot be judged
  required: true
  type: 15
  additionalinfo: Reliability of the source providing the intelligence data
- defaultvalue: indicatorType
  display: ""
  name: feedExpirationPolicy
  required: false
  type: 17
  options:
  - never
  - interval
  - indicatorType
  - suddenDeath
- display: ""
  name: feedExpirationInterval
  required: false
  type: 1
  defaultvalue: "20160"
- defaultvalue: '60'
  display: Feed Fetch Interval
  name: feedFetchInterval
  required: false
  type: 19
- additionalinfo: When selected, the exclusion list is ignored for indicators from this feed. This means that if an indicator from this feed is on the exclusion list, the indicator might still be added to the system.
  display: Bypass exclusion list
  name: feedBypassExclusionList
  required: false
  type: 8
  defaultvalue: ""
- additionalinfo: Time in seconds before http requests timeout.
  defaultvalue: '20'
  display: Request Timeout
  name: polling_timeout
  required: true
  type: 0
- display: Trust any certificate (not secure)
  name: insecure
  required: false
  type: 8
- display: Use system proxy settings
  name: proxy
  required: false
  type: 8
description: The SSL IP Blacklist contains all hosts (IP addresses) that SSLBL has seen in the past 30 days and identified as being associated with a malicious SSL certificate.
display: abuse.ch SSL Blacklist Feed
name: abuse.ch SSL Blacklist Feed
script:
  commands:
  - arguments:
    - default: false
      defaultValue: '50'
      description: The maximum number of results to return. The default value is 50.
      isArray: false
      name: limit
      required: false
      secret: false
    - default: false
      description: The indicator type.
      isArray: false
      name: indicator_type
      required: false
      secret: false
    deprecated: false
    description: Gets the feed indicators.
    execution: false
    name: sslbl-get-indicators
  dockerimage: demisto/python3:3.8.2.6981
  feed: true
  isfetch: false
  longRunning: false
  longRunningPort: false
  runonce: false
  script: >2



    def main():
        feed_url_to_config = {
            'https://sslbl.abuse.ch/blacklist/sslipblacklist.csv': {
                'fieldnames': ['firstseenbysource', 'value', 'port'],
                'indicator_type': FeedIndicatorType.IP,
                'mapping': {
                    'firstseenbysource': 'firstseenbysource',
                    'port': 'port'
                }
            },
            'https://sslbl.abuse.ch/blacklist/sslipblacklist_aggressive.csv': {
                'fieldnames': ['firstseenbysource', 'value', 'port'],
                'indicator_type': FeedIndicatorType.IP,
                'mapping': {
                    'firstseenbysource': 'firstseenbysource',
                    'port': 'port'
                }
            }
        }

        params = {k: v for k, v in demisto.params().items() if v is not None}
        params['feed_url_to_config'] = feed_url_to_config
        params['ignore_regex'] = r'^#'
        params['delimiter'] = ','

        # Main execution of the CSV API Module.
        # This function allows to add to or override this execution.
        feed_main('SSL Blacklist Feed', params, 'sslbl')



    ### GENERATED CODE ###

    # This code was inserted in place of an API module.




    ''' IMPORTS '''

    import csv

    import gzip

    import urllib3

    from dateutil.parser import parse

    from typing import Optional, Pattern, Dict, Any, Tuple, Union, List


    # disable insecure warnings

    urllib3.disable_warnings()


    # Globals



    class Client(BaseClient):
        def __init__(self, url: str, feed_url_to_config: Optional[Dict[str, dict]] = None, fieldnames: str = '',
                     insecure: bool = False, credentials: dict = None, ignore_regex: str = None, encoding: str = 'latin-1',
                     delimiter: str = ',', doublequote: bool = True, escapechar: str = '',
                     quotechar: str = '"', skipinitialspace: bool = False, polling_timeout: int = 20, proxy: bool = False,
                     feedTags: Optional[str] = None, **kwargs):
            """
            :param url: URL of the feed.
            :param feed_url_to_config: for each URL, a configuration of the feed that contains
             If *null* the values in the first row of the file are used as names. Default: *null*
             Example:
             feed_url_to_config = {
                'https://ipstack.com':
                {
                    'fieldnames': ['value'],
                    'indicator_type': 'IP',
                    'mapping': {
                        'Date': 'date' / 'Date': ('date', r'(regex_string)', 'The date is {}')
                    }
                }
             }
             For the mapping you can use either:
                1. 'indicator_field': 'value_from_feed'
                2. 'indicator_field': ('value_from_feed', regex_string_extractor, string_formatter)
                    * regex_string_extractor will extract the first match from the value_from_feed,
                    Use None to get the full value of the field.
                    * string_formatter will format the data in your preferred way, Use None to get the extracted field.
            :param fieldnames: list of field names in the file. If *null* the values in the first row of the file are
                used as names. Default: *null*
            :param insecure: boolean, if *false* feed HTTPS server certificate is verified. Default: *false*
            :param credentials: username and password used for basic authentication.
            Can be also used as API key header and value by specifying _header in the username field.
            :param ignore_regex: python regular expression for lines that should be ignored. Default: *null*
            :param encoding: Encoding of the feed, latin-1 by default.
            :param delimiter: see `csv Python module
                <https://docs.python.org/2/library/csv.html#dialects-and-formatting-parameters>`. Default: ,
            :param doublequote: see `csv Python module
                <https://docs.python.org/2/library/csv.html#dialects-and-formatting-parameters>`. Default: true
            :param escapechar: see `csv Python module
                <https://docs.python.org/2/library/csv.html#dialects-and-formatting-parameters>`. Default null
            :param quotechar: see `csv Python module
                <https://docs.python.org/2/library/csv.html#dialects-and-formatting-parameters>`. Default "
            :param skipinitialspace: see `csv Python module
                <https://docs.python.org/2/library/csv.html#dialects-and-formatting-parameters>`. Default False
            :param polling_timeout: timeout of the polling request in seconds. Default: 20
            :param proxy: Sets whether use proxy when sending requests
            """
            self.tags: List[str] = argToList(feedTags)
            if not credentials:
                credentials = {}

            auth: Optional[tuple] = None
            self.headers = {}

            username = credentials.get('identifier', '')
            if username.startswith('_header:'):
                header_name = username.split(':')[1]
                header_value = credentials.get('password', '')
                self.headers[header_name] = header_value
            else:
                password = credentials.get('password', '')
                auth = None
                if username is not None and password is not None:
                    auth = (username, password)

            super().__init__(base_url=url, proxy=proxy, verify=not insecure, auth=auth)

            try:
                self.polling_timeout = int(polling_timeout)
            except (ValueError, TypeError):
                return_error('Please provide an integer value for "Request Timeout"')
            self.encoding = encoding
            self.ignore_regex: Optional[Pattern] = None
            if ignore_regex is not None:
                self.ignore_regex = re.compile(ignore_regex)
            self.feed_url_to_config: Optional[Dict[str, dict]] = feed_url_to_config
            self.fieldnames = argToList(fieldnames)
            self.dialect: Dict[str, Any] = {
                'delimiter': delimiter,
                'doublequote': doublequote,
                'escapechar': escapechar,
                'quotechar': quotechar,
                'skipinitialspace': skipinitialspace
            }

        def _build_request(self, url):
            r = requests.Request(
                'GET',
                url,
                auth=self._auth
            )

            return r.prepare()

        def build_iterator(self, **kwargs):
            results = []
            urls = self._base_url
            if not isinstance(urls, list):
                urls = [urls]
            for url in urls:
                _session = requests.Session()

                prepreq = self._build_request(url)

                # this is to honour the proxy environment variables
                kwargs.update(_session.merge_environment_settings(
                    prepreq.url,
                    {}, None, None, None  # defaults
                ))
                kwargs['stream'] = True
                kwargs['verify'] = self._verify
                kwargs['timeout'] = self.polling_timeout

                if self.headers:
                    if 'headers' in kwargs:
                        kwargs['headers'].update(self.headers)
                    else:
                        kwargs['headers'] = self.headers

                try:
                    r = _session.send(prepreq, **kwargs)
                except requests.ConnectionError:
                    raise requests.ConnectionError('Failed to establish a new connection.'
                                                   ' Please make sure your URL is valid.')
                try:
                    r.raise_for_status()
                except Exception:
                    return_error('Exception in request: {} {}'.format(r.status_code, r.content))
                    raise

                response = self.get_feed_content_divided_to_lines(url, r)
                if self.feed_url_to_config:
                    fieldnames = self.feed_url_to_config.get(url, {}).get('fieldnames', [])
                else:
                    fieldnames = self.fieldnames
                if self.ignore_regex is not None:
                    response = filter(  # type: ignore
                        lambda x: self.ignore_regex.match(x) is None,  # type: ignore
                        response
                    )

                csvreader = csv.DictReader(
                    response,
                    fieldnames=fieldnames,
                    **self.dialect
                )

                results.append({url: csvreader})

            return results

        def get_feed_content_divided_to_lines(self, url, raw_response):
            """Fetch feed data and divides its content to lines

            Args:
                url: Current feed's url.
                raw_response: The raw response from the feed's url.

            Returns:
                List. List of lines from the feed content.
            """
            if self.feed_url_to_config and self.feed_url_to_config.get(url).get('is_zipped_file'):  # type: ignore
                response_content = gzip.decompress(raw_response.content)
            else:
                response_content = raw_response.content

            return response_content.decode(self.encoding).split('\n')


    def determine_indicator_type(indicator_type, default_indicator_type, auto_detect, value):
        """
        Detect the indicator type of the given value.
        Args:
            indicator_type: (str) Indicator type given in the config.
            default_indicator_type: Indicator type which was inserted as a param of the integration by user.
            auto_detect: (bool) True whether auto detection of the indicator type is wanted.
            value: (str) The value which we'd like to get indicator type of.
        Returns:
            Str which stands for the indicator type after detection.
        """
        if auto_detect:
            indicator_type = auto_detect_indicator_type(value)
        if not indicator_type:
            indicator_type = default_indicator_type
        return indicator_type


    def module_test_command(client: Client, args):
        client.build_iterator()
        return 'ok', {}, {}


    def date_format_parsing(date_string):
        formatted_date = parse(date_string).isoformat()
        if "+" in formatted_date:
            formatted_date = formatted_date.split('+')[0]

        if "." in formatted_date:
            formatted_date = formatted_date.split('.')[0]

        if not formatted_date.endswith('Z'):
            formatted_date = formatted_date + 'Z'

        return formatted_date


    def create_fields_mapping(raw_json: Dict[str, Any], mapping: Dict[str, Union[Tuple, str]]):
        fields_mapping = {}  # type: dict

        for key, field in mapping.items():
            regex_extractor = None
            formatter_string = None

            if isinstance(field, tuple):
                field, regex_extractor, formatter_string = field

            if not raw_json.get(field):  # type: ignore
                continue

            try:
                field_value = re.match(regex_extractor, raw_json[field]).group(1)  # type: ignore
            except Exception:
                field_value = raw_json[field]  # type: ignore

            fields_mapping[key] = formatter_string.format(field_value) if formatter_string else field_value

            if key in ['firstseenbysource', 'lastseenbysource']:
                fields_mapping[key] = date_format_parsing(fields_mapping[key])

        return fields_mapping


    def fetch_indicators_command(client: Client, default_indicator_type: str, auto_detect: bool, **kwargs):
        iterator = client.build_iterator(**kwargs)
        indicators = []
        config = client.feed_url_to_config or {}
        for url_to_reader in iterator:
            for url, reader in url_to_reader.items():
                mapping = config.get(url, {}).get('mapping', {})
                for item in reader:
                    raw_json = dict(item)
                    value = item.get('value')
                    if not value and len(item) > 1:
                        value = next(iter(item.values()))
                    if value:
                        raw_json['value'] = value
                        conf_indicator_type = config.get(url, {}).get('indicator_type')
                        indicator_type = determine_indicator_type(conf_indicator_type, default_indicator_type, auto_detect,
                                                                  value)
                        raw_json['type'] = indicator_type
                        indicator = {
                            'value': value,
                            'type': indicator_type,
                            'rawJSON': raw_json,
                            'fields': create_fields_mapping(raw_json, mapping) if mapping else {}
                        }
                        indicator['fields']['tags'] = client.tags
                        indicators.append(indicator)
        return indicators


    def get_indicators_command(client, args: dict, tags: Optional[List[str]] = None):
        if tags is None:
            tags = []
        itype = args.get('indicator_type', demisto.params().get('indicator_type'))
        try:
            limit = int(args.get('limit', 50))
        except ValueError:
            raise ValueError('The limit argument must be a number.')
        auto_detect = demisto.params().get('auto_detect_type')
        indicators_list = fetch_indicators_command(client, itype, auto_detect)
        entry_result = indicators_list[:limit]
        hr = tableToMarkdown('Indicators', entry_result, headers=['value', 'type', 'fields'])
        return hr, {}, indicators_list


    def feed_main(feed_name, params=None, prefix=''):
        if not params:
            params = {k: v for k, v in demisto.params().items() if v is not None}
        handle_proxy()
        client = Client(**params)
        command = demisto.command()
        if command != 'fetch-indicators':
            demisto.info('Command being called is {}'.format(command))
        if prefix and not prefix.endswith('-'):
            prefix += '-'
        # Switch case
        commands: dict = {
            'test-module': module_test_command,
            f'{prefix}get-indicators': get_indicators_command
        }
        try:
            if command == 'fetch-indicators':
                indicators = fetch_indicators_command(
                    client,
                    params.get('indicator_type'),
                    params.get('auto_detect_type')
                )
                # we submit the indicators in batches
                for b in batch(indicators, batch_size=2000):
                    demisto.createIndicators(b)  # type: ignore
            else:
                args = demisto.args()
                args['feed_name'] = feed_name
                readable_output, outputs, raw_response = commands[command](client, args)
                return_outputs(readable_output, outputs, raw_response)
        except Exception as e:
            err_msg = f'Error in {feed_name} Integration - Encountered an issue with createIndicators' if \
                'failed to create' in str(e) else f'Error in {feed_name} Integration [{e}]'
            return_error(err_msg)



    if __name__ == '__builtin__' or __name__ == 'builtins':
        main()
  subtype: python3
  type: python
fromversion: 5.5.0
image: data:image/png;base64,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
detaileddescription: >-
  sslipblacklist.csv:  

  The SSL IP Blacklist contains all hosts (IP addresses) that SSLBL has

  seen in the past 30 days being associated with a malicious SSL certificate.


  sslipblacklist_aggressive.csv:  

  The SSL IP Blacklist contains all hosts (IP addresses) that SSLBL has

  seen in the past being associated with a malicious SSL certificate. Warning - High FP Rate!
